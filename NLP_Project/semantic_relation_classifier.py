# -*- coding: utf-8 -*-
"""SemanticRelationClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wpmL-dcrLj_MmfmLqvzj3Q90bLYiWsCn
"""

# install necessary packages using pip
!pip install keras numpy wget
!pip install keras-self-attention

# imports and variables

import os
import sys
import numpy
import re
import itertools
import matplotlib
import matplotlib.pyplot as plt
import tensorflow as tf
import numpy as np

from google.colab import drive
drive.mount('/content/drive')
gdrive_path = '/content/drive/My Drive/'

wordToIdx = {}

relationToIdx = {'Cause-Effect': 0, 'Component-Whole': 1, 'Product-Producer': 2, 'Other-Relation': 3}
idxToRelation = ['Cause-Effect', 'Component-Whole', 'Product-Producer', 'Other-Relation']

def parse_sentence(sentence):
  punctuation = "!#$%&,-.:;?@{}"
  for punct in punctuation:
    sentence = sentence.replace(punct, " " + punct + " ")
  return sentence

def load_corpus(path):
    # Check if the path is a directory.
    sentences = []
    relations = []
    path = gdrive_path + path
    # TODO: Your code goes here
    with open(path, 'r') as f:
      for sentenceLine, relationLine, commentLine, newLine in itertools.zip_longest(*[f]*4):
        sentenceLine = sentenceLine[sentenceLine.index('\"')+1:sentenceLine.rindex('\"')]
        sentenceLine = sentenceLine.replace("<", " <")
        sentenceLine = sentenceLine.replace(">", "> ")
        sentenceLine = parse_sentence(sentenceLine)

        # Switch labels for backwards relations
        if 'e1' in relationLine and relationLine.index('e2') < relationLine.index('e1'):
          sentenceLine = sentenceLine.replace("<e1>", "E1START")
          sentenceLine = sentenceLine.replace("</e1>", "E1END")
          sentenceLine = sentenceLine.replace("<e2>", "<e1>")
          sentenceLine = sentenceLine.replace("</e2>", "</e1>")
          sentenceLine = sentenceLine.replace("E1START", "<e2>")
          sentenceLine = sentenceLine.replace("E1END", "</e2>")
    
        sentences.append(sentenceLine)

        if 'Cause-Effect' in relationLine: 
          relations.append(relationToIdx['Cause-Effect'])
        elif 'Component-Whole' in relationLine:
          relations.append(relationToIdx['Component-Whole'])
        elif 'Product-Producer' in relationLine:
          relations.append(relationToIdx['Product-Producer'])
        else:
          relations.append(relationToIdx['Other-Relation'])
      return sentences, relations

def create_dataset(train_X):
  X = list()
  wordToIdx["PAD"] = 0
  wordToIdx["OOV"] = 1
  wordIdx = 2
  for sentence in train_X:
    curX = []
    for word in sentence.split():
      if word not in wordToIdx:
        wordToIdx[word] = wordIdx
        wordIdx += 1
      curX.append(wordToIdx[word])
    X.append(numpy.array(curX))
  return numpy.array(X, dtype=object)

def create_test_dataset(test_X):
  X = list()
  for sentence in test_X:
    curX = []
    for word in sentence.split():
      if word not in wordToIdx:
        curX.append(wordToIdx["OOV"])
      else:
        curX.append(wordToIdx[word])
    X.append(numpy.array(curX))
  return numpy.array(X, dtype=object)

from keras_preprocessing.sequence import pad_sequences as pad

# Pad the sequences with 0s to the max length.
def pad_sequences(train_X):
    # Use MAX_LENGTH to record length of longest sequence 
    maxSentences = None
    train_X = pad(train_X, padding="post")
    MAX_LENGTH = len(train_X[0])
    return train_X, MAX_LENGTH

# Returns the one-hot encoding of the sequence.
from keras.utils import np_utils
def to_categorical(train_y):
  train_y = np_utils.to_categorical(train_y)
  return train_y

# Keras Model
from keras.models import Sequential
from keras.layers import Dense, LSTM, Activation, InputLayer, Bidirectional, Embedding
from keras.optimizers import Adam
from keras_self_attention import SeqSelfAttention, SeqWeightedAttention
from keras import activations
from keras.layers import Flatten
# Define the Keras model.
def define_model(MAX_LENGTH):  
  embedding_size = 300
  model = Sequential()
  model.add(Embedding(len(wordToIdx), embedding_size, input_length=train_X.shape[1]))
  model.add(Bidirectional(LSTM(128, dropout=0.7, recurrent_dropout=0.7, return_sequences=True)))
  model.add(SeqSelfAttention(attention_activation='sigmoid'))
  model.add(Flatten())
  model.add(Dense(train_y.shape[1], input_shape = (MAX_LENGTH,), activation='softmax'))
  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])
  print(model.summary())
  return model

# Trains the model.
def train(model, train_X, train_y, Epochs):
    # Fit the data into the Keras model
    model.fit(train_X, train_y, epochs=Epochs, batch_size=128, validation_split=0.2) 
    return model

# Test the model
def test(model, test_X):
  predictions = model.predict(test_X)
  return predictions

def uncategorize(sequences):
  token_sequence = []
  for categorical in sequences:
    token_sequence.append(idxToRelation[np.argmax(categorical)])
  return token_sequence

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
# main

wordToIdx.clear()

sentences, relations = load_corpus("dataset/SemEval2010_task8_all_data/SemEval2010_task8_training/TRAIN_FILE.TXT")
testSentences, testRelations = load_corpus("dataset/SemEval2010_task8_all_data/SemEval2010_task8_testing_keys/TEST_FILE_FULL.TXT")

# Train Data Statistics
sum = len(relations)
print("Total :", sum)
for relation in idxToRelation:
  if relation == 'PAD':
    continue
  num = relations.count(relationToIdx[relation])
  percent = 100*num/sum
  print(relation, ":", num,  f"({percent:.2f}%)")
print()

# Process Sentence Input
train_X = create_dataset(sentences)
test_X = create_test_dataset(testSentences)
train_y = numpy.array(relations)

train_X = pad(train_X, padding="post")
MAX_LENGTH = len(train_X[0])

test_X = pad(test_X, maxlen=MAX_LENGTH, padding="post")
train_y = to_categorical(train_y)

# Define Model
model = define_model(MAX_LENGTH)
print()

# Train model
model = train(model, train_X, train_y, 20)

# Test model
predictions = test(model, test_X)
test_Y = uncategorize(predictions)
gold_standard = [idxToRelation[i] for i in testRelations]

# Test model statistics
print(confusion_matrix(test_Y, gold_standard)) 
print(classification_report(test_Y, gold_standard, digits=3))

# Show testing on 50 example sentences
n = len(test_Y)
sampleIdxs = np.random.randint(0, high=n, size=50)
count = 0
for idx in sampleIdxs:
  print("ID:", 8001+idx, "in test file")
  print("Sentence:", testSentences[idx])
  print("Predicted:", test_Y[idx])
  print("Actual:", gold_standard[idx])
  print()
  if test_Y[idx] == gold_standard[idx]:
    count += 1
print("Accuracy for the current 50:", f"({100*count/50:.2f}%)")
